{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "Adapted from https://github.com/gpeyre/SinkhornAutoDiff\n",
    "and from https://github.com/dfdazac/wassdistance/blob/master/layers.py\n",
    "and from https://github.com/michaelsdr/sinkformers/blob/main/nlp-tutorial/text-classification-transformer/sinkhorn.py\n",
    "\"\"\"\n",
    "\n",
    "def shape_list(x, out_type=tf.int32):\n",
    "  \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "  static = x.shape.as_list()\n",
    "  dynamic = tf.shape(x, out_type=out_type)\n",
    "  return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "def sinkhorn_distance(input_tensor, eps, max_iter, \n",
    "                  reduction='none',\n",
    "                  stopThr=1e-2):\n",
    "  \n",
    "  C = input_tensor\n",
    "  C_shape = shape_list(C)\n",
    "\n",
    "  x_points = C_shape[-2]\n",
    "  y_points = C_shape[-1]\n",
    "  batch_size = C_shape[0]\n",
    "    \n",
    "  # both marginals are fixed with equal weights\n",
    "  mu = 1.0 / x_points * tf.ones((batch_size, x_points))\n",
    "  nu = 1.0 / y_points * tf.ones((batch_size, y_points))\n",
    "\n",
    "  u = tf.zeros_like(mu)\n",
    "  v = tf.zeros_like(nu)\n",
    "\n",
    "  cpt = tf.constant(0)\n",
    "  err = tf.constant(1.0)\n",
    "\n",
    "  c = lambda cpt, u, v, err: tf.logical_and(cpt < max_iter, err > stopThr)\n",
    "\n",
    "  def M( C, u, v):\n",
    "    \"Modified cost for logarithmic updates\"\n",
    "    \"$M_{ij} = (-c_{ij} + u_i + v_j) / \\epsilon$\"\n",
    "    return (-C + tf.expand_dims(u, -1) + tf.expand_dims(v, -2) )/eps\n",
    "\n",
    "  def loop_func(cpt, u, v, err):\n",
    "    u1 = tf.identity(u)  # useful to check the update\n",
    "\n",
    "    cpt = cpt + 1\n",
    "\n",
    "    u = eps * (tf.log(mu+1e-8) - tf.reduce_logsumexp(M(C, u, v), axis=-1)) + u\n",
    "    v = eps * (tf.log(nu+1e-8) - tf.reduce_logsumexp(tf.transpose(M(C, u, v), [0, 2, 1]), axis=-1)) + v\n",
    "\n",
    "    err = tf.reduce_mean(tf.reduce_sum(tf.abs(u - u1), axis=-1))\n",
    "\n",
    "    return cpt, u, v, err\n",
    "\n",
    "  _, u_final, v_final, _ = tf.while_loop(c, loop_func, loop_vars=[cpt, u, v, err])\n",
    "  U, V = tf.identity(u_final), tf.identity(v_final)\n",
    "\n",
    "  # Transport plan pi = diag(a)*K*diag(b)\n",
    "  pi = tf.exp(M(C, U, V))\n",
    "\n",
    "  cost = tf.reduce_sum(pi * C, axis=(-2, -1))\n",
    "\n",
    "  return pi, C, U, V, cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1.0\n",
    "max_iter = 10\n",
    "stopThr = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cost_matrix(x, y, p=2):\n",
    "    \"Returns the matrix of $|x_i-y_j|^p$.\"\n",
    "    x_col = tf.expand_dims(x, axis=-2)\n",
    "    y_lin = tf.expand_dims(y, axis=-3)\n",
    "    C = tf.reduce_sum((tf.abs(x_col - y_lin)) ** p, -1)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((1, 10, 32)).astype(np.float32)\n",
    "y = np.random.random((1, 16, 32)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = _cost_matrix(tf.constant(x), tf.constant(y), p=2)\n",
    "\n",
    "[pi, C_, U, V, final_cost] = sinkhorn_distance(C, eps, max_iter, \n",
    "                  reduction='none',\n",
    "                  stopThr=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "resp = sess.run([pi, C, U, V, final_cost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp[0].sum(axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Adapted from https://github.com/gpeyre/SinkhornAutoDiff\n",
    "class SinkhornDistance(nn.Module):\n",
    "    r\"\"\"\n",
    "    Given two empirical measures each with :math:`P_1` locations\n",
    "    :math:`x\\in\\mathbb{R}^{D_1}` and :math:`P_2` locations :math:`y\\in\\mathbb{R}^{D_2}`,\n",
    "    outputs an approximation of the regularized OT cost for point clouds.\n",
    "    Args:\n",
    "        eps (float): regularization coefficient\n",
    "        max_iter (int): maximum number of Sinkhorn iterations\n",
    "        reduction (string, optional): Specifies the reduction to apply to the output:\n",
    "            'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n",
    "            'mean': the sum of the output will be divided by the number of\n",
    "            elements in the output, 'sum': the output will be summed. Default: 'none'\n",
    "    Shape:\n",
    "        - Input: :math:`(N, P_1, D_1)`, :math:`(N, P_2, D_2)`\n",
    "        - Output: :math:`(N)` or :math:`()`, depending on `reduction`\n",
    "    \"\"\"\n",
    "    def __init__(self, eps, max_iter, reduction='none'):\n",
    "        super(SinkhornDistance, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.max_iter = max_iter\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # The Sinkhorn algorithm takes as input three variables :\n",
    "        C = self._cost_matrix(x, y)  # Wasserstein cost function\n",
    "        x_points = x.shape[-2]\n",
    "        y_points = y.shape[-2]\n",
    "        if x.dim() == 2:\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = x.shape[0]\n",
    "\n",
    "        # both marginals are fixed with equal weights\n",
    "        mu = torch.empty(batch_size, x_points, dtype=torch.float,\n",
    "                         requires_grad=False).fill_(1.0 / x_points).squeeze()\n",
    "        nu = torch.empty(batch_size, y_points, dtype=torch.float,\n",
    "                         requires_grad=False).fill_(1.0 / y_points).squeeze()\n",
    "\n",
    "        u = torch.zeros_like(mu)\n",
    "        v = torch.zeros_like(nu)\n",
    "        # To check if algorithm terminates because of threshold\n",
    "        # or max iterations reached\n",
    "        actual_nits = 0\n",
    "        # Stopping criterion\n",
    "        thresh = 1e-1\n",
    "\n",
    "        # Sinkhorn iterations\n",
    "        for i in range(self.max_iter):\n",
    "            u1 = u  # useful to check the update\n",
    "            u = self.eps * (torch.log(mu+1e-8) - torch.logsumexp(self.M(C, u, v), dim=-1)) + u\n",
    "            v = self.eps * (torch.log(nu+1e-8) - torch.logsumexp(self.M(C, u, v).transpose(-2, -1), dim=-1)) + v\n",
    "            err = (u - u1).abs().sum(-1).mean()\n",
    "\n",
    "            actual_nits += 1\n",
    "            if err.item() < thresh:\n",
    "                break\n",
    "\n",
    "        U, V = u, v\n",
    "        # Transport plan pi = diag(a)*K*diag(b)\n",
    "        pi = torch.exp(self.M(C, U, V))\n",
    "        # Sinkhorn distance\n",
    "        cost = torch.sum(pi * C, dim=(-2, -1))\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            cost = cost.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            cost = cost.sum()\n",
    "\n",
    "        return cost, pi, C\n",
    "\n",
    "    def M(self, C, u, v):\n",
    "        \"Modified cost for logarithmic updates\"\n",
    "        \"$M_{ij} = (-c_{ij} + u_i + v_j) / \\epsilon$\"\n",
    "        return (-C + u.unsqueeze(-1) + v.unsqueeze(-2)) / self.eps\n",
    "\n",
    "    @staticmethod\n",
    "    def _cost_matrix(x, y, p=2):\n",
    "        \"Returns the matrix of $|x_i-y_j|^p$.\"\n",
    "        x_col = x.unsqueeze(-2)\n",
    "        y_lin = y.unsqueeze(-3)\n",
    "        C = torch.sum((torch.abs(x_col - y_lin)) ** p, -1)\n",
    "        return C\n",
    "\n",
    "    @staticmethod\n",
    "    def ave(u, u1, tau):\n",
    "        \"Barycenter subroutine, used by kinetic acceleration through extrapolation.\"\n",
    "        return tau * u + (1 - tau) * u1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sink = SinkhornDistance(eps=eps, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cost, pi, C = sink.forward(torch.tensor(x), torch.tensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "      CharBPETokenizer,\n",
    "      SentencePieceBPETokenizer,\n",
    "      BertWordPieceTokenizer)\n",
    "\n",
    "vocab = '/data/xuht/uncased_L-12_H-768_A-12_ilm_v1/vocab_uncased_en.txt'\n",
    "\n",
    "chinese_bpe_tokenizer = BertWordPieceTokenizer(\n",
    "    vocab, \n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(chinese_bpe_tokenizer.decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.not_equal([1.,2.,3.,0.], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "a = [1,0,3]\n",
    "label_binarizer = sklearn.preprocessing.LabelBinarizer()\n",
    "label_binarizer.fit(range(max(a)+1))\n",
    "b = label_binarizer.transform(a)\n",
    "print('{0}'.format(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.random.randint(1, 4, size=[2,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tf = tf.one_hot(label, depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_label = sess.run(label_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(one_hot_label.sum(axis=0) !=0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_relative_positions_matrix_t5(length, max_relative_position,\n",
    "                                        num_buckets=32,\n",
    "                                        bidirectional=True):\n",
    "  \n",
    "  \"\"\"\n",
    "  https://github.com/bojone/bert4keras/blob/master/bert4keras/layers.py\n",
    "  https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py\n",
    "  # _relative_position_bucket\n",
    "  https://gist.github.com/huchenxucs/c65524185e8e35c4bcfae4059f896c16\n",
    "  \"\"\"\n",
    "\n",
    "  tf.logging.info(\"** apply all distance mat **\")\n",
    "  range_vec = tf.range(length)\n",
    "\n",
    "  q_idxs = tf.expand_dims(range_vec, 1)\n",
    "  v_idxs = tf.expand_dims(range_vec, 0)\n",
    "\n",
    "  distance_mat = v_idxs - q_idxs  \n",
    "  # range_mat = tf.reshape(tf.tile(range_vec, [length]), [length, length])\n",
    "  # distance_mat = range_mat - tf.transpose(range_mat)\n",
    "    \n",
    "  num_buckets = num_buckets\n",
    "  max_distance = max_relative_position\n",
    "  ret = 0\n",
    "  n = -distance_mat\n",
    "  if bidirectional:\n",
    "    num_buckets //= 2\n",
    "    ret += tf.cast(tf.less(n, 0), 'int32') * num_buckets\n",
    "    n = tf.abs(n)\n",
    "  else:\n",
    "    n = tf.maximum(n, 0)\n",
    "  # now n is in the range [0, inf)\n",
    "  max_exact = num_buckets // 2\n",
    "  is_small = tf.less(n, max_exact)\n",
    "  val_if_large = max_exact + tf.cast(\n",
    "      tf.log(tf.cast(n, dtype=tf.float32) / max_exact) /\n",
    "      tf.log(max_distance / max_exact) * (num_buckets - max_exact),\n",
    "      'int32',\n",
    "  )\n",
    "  val_if_large = tf.minimum(val_if_large, num_buckets - 1)\n",
    "  tf_switch = (tf.cast(is_small, dtype=tf.int32)) * n + (1-tf.cast(is_small, dtype=tf.int32)) * val_if_large\n",
    "  ret += tf_switch #tf.switch(is_small, n, val_if_large)\n",
    "  # ret += tf.where(is_small, n, val_if_large)\n",
    "\n",
    "  return ret\n",
    "\n",
    "length=64\n",
    "max_relative_position=32\n",
    "num_buckets=32\n",
    "bidirectional=True\n",
    "\n",
    "ret_bi = _generate_relative_positions_matrix_t5(length, max_relative_position,\n",
    "                                        num_buckets=num_buckets,\n",
    "                                        bidirectional=bidirectional)\n",
    "\n",
    "ret_uni = _generate_relative_positions_matrix_t5(length, max_relative_position,\n",
    "                                        num_buckets=num_buckets,\n",
    "                                        bidirectional=False)\n",
    "\n",
    "ret = sess.run([ret_bi, ret_uni])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _relative_position_bucket_(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n",
    "        \"\"\"\n",
    "        Adapted from Mesh Tensorflow:\n",
    "        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n",
    "        Translate relative position to a bucket number for relative attention.\n",
    "        The relative position is defined as memory_position - query_position, i.e.\n",
    "        the distance in tokens from the attending position to the attended-to\n",
    "        position.  If bidirectional=False, then positive relative positions are\n",
    "        invalid.\n",
    "        We use smaller buckets for small absolute relative_position and larger buckets\n",
    "        for larger absolute relative_positions.  All relative positions >=max_distance\n",
    "        map to the same bucket.  All relative positions <=-max_distance map to the\n",
    "        same bucket.  This should allow for more graceful generalization to longer\n",
    "        sequences than the model has been trained on.\n",
    "        Args:\n",
    "            relative_position: an int32 Tensor\n",
    "            bidirectional: a boolean - whether the attention is bidirectional\n",
    "            num_buckets: an integer\n",
    "            max_distance: an integer\n",
    "        Returns:\n",
    "            a Tensor with the same shape as relative_position, containing int32\n",
    "            values in the range [0, num_buckets)\n",
    "        \"\"\"\n",
    "        ret = 0\n",
    "        n = -relative_position\n",
    "        if bidirectional:\n",
    "            num_buckets //= 2\n",
    "            ret += (n < 0).to(torch.long) * num_buckets  # mtf.to_int32(mtf.less(n, 0)) * num_buckets\n",
    "            n = torch.abs(n)\n",
    "        else:\n",
    "            n = torch.max(n, torch.zeros_like(n))\n",
    "        # now n is in the range [0, inf)\n",
    "\n",
    "        # half of the buckets are for exact increments in positions\n",
    "        max_exact = num_buckets // 2\n",
    "        is_small = n < max_exact\n",
    "\n",
    "        # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n",
    "        val_if_large = max_exact + (\n",
    "            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n",
    "        ).to(torch.long)\n",
    "        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n",
    "\n",
    "        ret += torch.where(is_small, n, val_if_large)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_position = torch.arange(64, dtype=torch.long)[:, None]\n",
    "memory_position = torch.arange(64, dtype=torch.long)[None, :]\n",
    "relative_position = memory_position - context_position  # shape (qlen, klen)\n",
    "resp = _relative_position_bucket_(relative_position, bidirectional=False, num_buckets=32, max_distance=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[0][25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1-segment_ids) * resp[25].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_position = torch.arange(64, dtype=torch.long)[:, None]\n",
    "memory_position = torch.arange(64, dtype=torch.long)[None, :]\n",
    "relative_position = memory_position - context_position  # shape (qlen, klen)\n",
    "s1 = _relative_position_bucket_(relative_position, bidirectional=True, num_buckets=32, max_distance=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_position = torch.arange(64, dtype=torch.long)[:, None]\n",
    "memory_position = torch.arange(64, dtype=torch.long)[None, :]\n",
    "relative_position = memory_position - context_position  # shape (qlen, klen)\n",
    "s3 = _relative_position_bucket_(relative_position, bidirectional=False, num_buckets=32, max_distance=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_position = torch.arange(32, dtype=torch.long)[:, None]\n",
    "memory_position = torch.arange(32, dtype=torch.long)[None, :]\n",
    "relative_position = memory_position - context_position  # shape (qlen, klen)\n",
    "s2 = _relative_position_bucket_(relative_position, bidirectional=True, num_buckets=32, max_distance=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1[31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = s1 * (1-segment_ids[None, :]) * (1-segment_ids[:, None]) + s3 * (segment_ids[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "segment_ids = [0]*25+[1]*39\n",
    "\n",
    "segment_mask = tf.cast(np.array([segment_ids, segment_ids]), dtype=tf.int32)\n",
    "relative_positions_matrix_bi = tf.constant(ret[0])\n",
    "relative_positions_matrix_uni = tf.constant(ret[1])\n",
    "\n",
    "# handle mixture of bi and uni-direction relative position\n",
    "# [1, seq_len, seq_len]\n",
    "relative_positions_matrix_bi = tf.expand_dims(relative_positions_matrix_bi, axis=0)\n",
    "relative_positions_matrix_uni = tf.expand_dims(relative_positions_matrix_uni, axis=0)\n",
    "\n",
    "# s1 * (1-segment_ids[None, :]) * (1-segment_ids[:, None]) + s3 * (segment_ids[:, None])\n",
    "# [batch, seq_len, seq_len]\n",
    "relative_positions_matrix = relative_positions_matrix_bi * (1-tf.expand_dims(segment_mask, axis=1)) * (1-tf.expand_dims(segment_mask, axis=-1)) + relative_positions_matrix_uni * (tf.expand_dims(segment_mask, axis=-1))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = sess.run(relative_positions_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "63356*768-21228*512*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "init_np = np.random.random((4, 2, 3))\n",
    "update_np = np.random.random((1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    with tf.variable_scope(\"test\", reuse=tf.AUTO_REUSE):\n",
    "        queue = tf.get_variable('queue', \n",
    "                      [4, 2, 3], \n",
    "                      dtype=tf.float32,\n",
    "                      initializer=tf.constant_initializer(-1e10),\n",
    "                      trainable=False)\n",
    "    \n",
    "    \n",
    "    sess = tf.Session()\n",
    "    queue_op = queue.assign(tf.concat([tf.constant((update_np+np.random.random((1, 2, 3))).astype(np.float32)), queue[:-1, :, :]], axis=0))\n",
    "    with tf.control_dependencies([queue_op]):\n",
    "        loss = tf.reduce_sum(queue)\n",
    "    #     p = queue + 1\n",
    "        f = tf.identity(queue)\n",
    "        Z = tf.reduce_logsumexp(queue, axis=0)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-180000000000.0\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    print(sess.run(loss))\n",
    "#     print(sess.run(f))\n",
    "#     print(sess.run(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 2]\n",
      "[5, 4, 1]\n",
      "[6, 5, 4]\n",
      "[6, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "for i in [4,5,6]:\n",
    "    a = [i]+a[:-1]\n",
    "    print(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue_mask = tf.cast(tf.not_equal(queue, 0), dtype=tf.float32)\n",
    "Z = tf.reduce_logsumexp(queue-(1-queue_mask)*1e10, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.random((2, 3)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1.0,0.0], [0.0,1.0]]).astype(np.float32)\n",
    "with graph.as_default():\n",
    "    resp = (sess.run(tf.einsum('nc,ck->nck', tf.constant(a), tf.constant(b))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9684313 , 0.755596  , 0.557807  ],\n",
       "       [0.70687485, 0.7513403 , 0.15060379]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2902815 , 1.1406752 , 1.0104504 ],\n",
       "       [1.1077851 , 1.1377815 , 0.77128166]], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(np.exp(resp).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9684313 , 0.755596  , 0.557807  ],\n",
       "       [0.70687485, 0.7513403 , 0.15060379]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2902817 1.1406751 1.0104502]\n",
      " [1.107785  1.1377815 0.7712816]]\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    print(sess.run(tf.reduce_logsumexp(tf.einsum('nc,ck->nck', tf.constant(a), tf.constant(b)), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20927122 0.27338809 0.3571492  0.16019149]]\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    print(sess.run(tf.nn.softmax(tf.nn.l2_normalize(np.array([[1., 2., 3., 0.]]), axis=-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor(reader, src_name):\n",
    "  tensor = reader.get_tensor(src_name)\n",
    "  return tensor, src_name\n",
    "\n",
    "reader = tf.train.load_checkpoint('/data/xuht/roberta_base/roberta_base/roberta_base.ckpt')\n",
    "var_values, var_dtypes = {}, {}\n",
    "import numpy as np\n",
    "for (name, _) in tf.train.list_variables('/data/xuht/roberta_base/roberta_base/roberta_base.ckpt'):\n",
    "  # skip global_step and optimizer states in src ckpt if not FLAGS.retain_all\n",
    "\n",
    "  tensor, tgt_name = get_tensor(reader, name)\n",
    "  var_values[tgt_name] = np.array(tensor).astype(np.float32)\n",
    "  var_dtypes[tgt_name] = tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'additional_emb': dtype('float32'),\n",
       " 'lm_head/bias': dtype('float32'),\n",
       " 'lm_head/decoder/bias': dtype('float32'),\n",
       " 'lm_head/decoder/kernel': dtype('float32'),\n",
       " 'lm_head/dense/bias': dtype('float32'),\n",
       " 'lm_head/dense/kernel': dtype('float32'),\n",
       " 'lm_head/layer_norm/bias': dtype('float32'),\n",
       " 'lm_head/layer_norm/kernel': dtype('float32'),\n",
       " 'roberta/embeddings/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/embeddings/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/embeddings/position_embeddings': dtype('float32'),\n",
       " 'roberta/embeddings/position_ids': dtype('int64'),\n",
       " 'roberta/embeddings/token_type_embeddings': dtype('float32'),\n",
       " 'roberta/embeddings/word_embeddings': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/attention/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/attention/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/attention/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/attention/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/attention/self/key/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/attention/self/key/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/attention/self/query/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/attention/self/query/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/attention/self/value/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/attention/self/value/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/intermediate/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/intermediate/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_0/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/attention/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/attention/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/attention/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/attention/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/attention/self/key/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/attention/self/key/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/attention/self/query/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/attention/self/query/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/attention/self/value/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/attention/self/value/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/intermediate/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/intermediate/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_1/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/attention/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/attention/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/attention/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/attention/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/attention/self/key/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/attention/self/key/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/attention/self/query/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/attention/self/query/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/attention/self/value/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/attention/self/value/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/intermediate/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/intermediate/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_10/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/attention/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/attention/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/attention/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/attention/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/attention/self/key/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/attention/self/key/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/attention/self/query/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/attention/self/query/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/attention/self/value/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/attention/self/value/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/intermediate/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/intermediate/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_11/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/attention/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/attention/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/attention/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/attention/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/attention/self/key/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/attention/self/key/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/attention/self/query/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/attention/self/query/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/attention/self/value/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/attention/self/value/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/intermediate/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/intermediate/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_2/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/attention/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/attention/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/attention/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/attention/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/attention/self/key/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/attention/self/key/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/attention/self/query/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/attention/self/query/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/attention/self/value/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/attention/self/value/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/intermediate/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/intermediate/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_3/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/attention/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/attention/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/attention/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/attention/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/attention/self/key/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/attention/self/key/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/attention/self/query/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/attention/self/query/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/attention/self/value/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/attention/self/value/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/intermediate/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/intermediate/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_4/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/attention/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/attention/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/attention/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/attention/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/attention/self/key/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/attention/self/key/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/attention/self/query/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/attention/self/query/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/attention/self/value/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/attention/self/value/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/intermediate/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/intermediate/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_5/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/attention/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/attention/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/attention/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/attention/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/attention/self/key/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/attention/self/key/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/attention/self/query/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/attention/self/query/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/attention/self/value/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/attention/self/value/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/intermediate/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/intermediate/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_6/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/attention/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/attention/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/attention/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/attention/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/attention/self/key/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/attention/self/key/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/attention/self/query/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/attention/self/query/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/attention/self/value/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/attention/self/value/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/intermediate/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/intermediate/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_7/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/attention/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/attention/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/attention/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/attention/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/attention/self/key/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/attention/self/key/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/attention/self/query/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/attention/self/query/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/attention/self/value/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/attention/self/value/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/intermediate/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/intermediate/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_8/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/attention/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/attention/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/attention/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/attention/output/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/attention/self/key/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/attention/self/key/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/attention/self/query/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/attention/self/query/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/attention/self/value/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/attention/self/value/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/intermediate/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/intermediate/dense/kernel': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/output/LayerNorm/beta': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/output/LayerNorm/gamma': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/output/dense/bias': dtype('float32'),\n",
       " 'roberta/encoder/layer_9/output/dense/kernel': dtype('float32')}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00720883, -0.03820204, -0.00852567, ..., -0.0142212 ,\n",
       "         0.00777425,  0.02203818],\n",
       "       [-0.00958817,  0.01995133, -0.00972462, ..., -0.00903561,\n",
       "         0.02401158, -0.01534905],\n",
       "       [ 0.01020542, -0.01633411, -0.00990577, ..., -0.01082997,\n",
       "        -0.00271305,  0.00275441],\n",
       "       ...,\n",
       "       [ 0.01659554,  0.01053478, -0.00044641, ..., -0.00437227,\n",
       "        -0.00624541,  0.02083383],\n",
       "       [-0.0223963 ,  0.00948278,  0.00989949, ..., -0.00331239,\n",
       "         0.01337678,  0.00422556],\n",
       "       [-0.00702481,  0.00379791, -0.00395953, ..., -0.00576761,\n",
       "        -0.02389995,  0.00698871]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_values['additional_emb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14758301, -0.03649902,  0.07531738, ..., -0.00227928,\n",
       "         0.01724243, -0.00158501],\n",
       "       [ 0.015625  ,  0.00759125, -0.01183319, ..., -0.00222015,\n",
       "         0.00807953, -0.015625  ],\n",
       "       [-0.034729  , -0.08728027, -0.01800537, ...,  0.11743164,\n",
       "        -0.0098114 , -0.03549194],\n",
       "       ...,\n",
       "       [ 0.03044128,  0.05044556, -0.03068542, ...,  0.03768921,\n",
       "         0.00956726,  0.00836182],\n",
       "       [ 0.06228638, -0.05960083,  0.03071594, ..., -0.09197998,\n",
       "         0.10803223, -0.01832581],\n",
       "       [ 0.12585449, -0.01449585,  0.03317261, ...,  0.01206207,\n",
       "         0.03421021,  0.0168457 ]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_values['lm_head/decoder/kernel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14758301, -0.03649902,  0.07531738, ..., -0.00227928,\n",
       "         0.01724243, -0.00158501],\n",
       "       [ 0.015625  ,  0.00759125, -0.01183319, ..., -0.00222015,\n",
       "         0.00807953, -0.015625  ],\n",
       "       [-0.034729  , -0.08728027, -0.01800537, ...,  0.11743164,\n",
       "        -0.0098114 , -0.03549194],\n",
       "       ...,\n",
       "       [ 0.01659554,  0.01053478, -0.00044641, ..., -0.00437227,\n",
       "        -0.00624541,  0.02083383],\n",
       "       [-0.0223963 ,  0.00948278,  0.00989949, ..., -0.00331239,\n",
       "         0.01337678,  0.00422556],\n",
       "       [-0.00702481,  0.00379791, -0.00395953, ..., -0.00576761,\n",
       "        -0.02389995,  0.00698871]], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_values['roberta/embeddings/word_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(input_file, quotechar=None, max_lines=None):\n",
    "  \"\"\"Reads a tab separated value file.\"\"\"\n",
    "  with tf.io.gfile.GFile(input_file, \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "    lines = []\n",
    "    for i, line in enumerate(reader):\n",
    "      if max_lines and i >= max_lines:\n",
    "        break\n",
    "      lines.append(line)\n",
    "    return lines\n",
    "import csv\n",
    "\n",
    "lines = read_tsv('/data/xuht/glue/MNLI/dev_matched.tsv', max_lines=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file comes originally from https://github.com/google-research/bert/blob/master/tokenization.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import re\n",
    "import unicodedata\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n",
    "  \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n",
    "\n",
    "  # The casing has to be passed in by the user and there is no explicit check\n",
    "  # as to whether it matches the checkpoint. The casing information probably\n",
    "  # should have been stored in the bert_config.json file, but it's not, so\n",
    "  # we have to heuristically detect it to validate.\n",
    "\n",
    "  if not init_checkpoint:\n",
    "    return\n",
    "\n",
    "  m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n",
    "  if m is None:\n",
    "    return\n",
    "\n",
    "  model_name = m.group(1)\n",
    "\n",
    "  lower_models = [\n",
    "      \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n",
    "      \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n",
    "  ]\n",
    "\n",
    "  cased_models = [\n",
    "      \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n",
    "      \"multi_cased_L-12_H-768_A-12\"\n",
    "  ]\n",
    "\n",
    "  is_bad_config = False\n",
    "  if model_name in lower_models and not do_lower_case:\n",
    "    is_bad_config = True\n",
    "    actual_flag = \"False\"\n",
    "    case_name = \"lowercased\"\n",
    "    opposite_flag = \"True\"\n",
    "\n",
    "  if model_name in cased_models and do_lower_case:\n",
    "    is_bad_config = True\n",
    "    actual_flag = \"True\"\n",
    "    case_name = \"cased\"\n",
    "    opposite_flag = \"False\"\n",
    "\n",
    "  if is_bad_config:\n",
    "    raise ValueError(\n",
    "        \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n",
    "        \"However, `%s` seems to be a %s model, so you \"\n",
    "        \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n",
    "        \"how the model was pre-training. If this error is wrong, please \"\n",
    "        \"just comment out this check.\" % (actual_flag, init_checkpoint,\n",
    "                                          model_name, case_name, opposite_flag))\n",
    "\n",
    "\n",
    "def convert_to_unicode(text):\n",
    "  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "  if six.PY3:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, bytes):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  elif six.PY2:\n",
    "    if isinstance(text, str):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    elif isinstance(text, unicode):\n",
    "      return text\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  else:\n",
    "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "\n",
    "\n",
    "def printable_text(text):\n",
    "  \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
    "\n",
    "  # These functions want `str` for both Python2 and Python3, but in one case\n",
    "  # it's a Unicode string and in the other it's a byte string.\n",
    "  if six.PY3:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, bytes):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  elif six.PY2:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, unicode):\n",
    "      return text.encode(\"utf-8\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  else:\n",
    "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "  vocab = collections.OrderedDict()\n",
    "  index = 0\n",
    "  with tf.gfile.GFile(vocab_file, \"r\") as reader:\n",
    "    while True:\n",
    "      token = convert_to_unicode(reader.readline())\n",
    "      if not token:\n",
    "        break\n",
    "      token = token.strip()\n",
    "      vocab[token] = index\n",
    "      index += 1\n",
    "  return vocab\n",
    "\n",
    "\n",
    "def convert_by_vocab(vocab, items):\n",
    "  \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n",
    "  output = []\n",
    "  for item in items:\n",
    "    output.append(vocab[item])\n",
    "  return output\n",
    "\n",
    "\n",
    "def convert_tokens_to_ids(vocab, tokens):\n",
    "  return convert_by_vocab(vocab, tokens)\n",
    "\n",
    "\n",
    "def convert_ids_to_tokens(inv_vocab, ids):\n",
    "  return convert_by_vocab(inv_vocab, ids)\n",
    "\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "  text = text.strip()\n",
    "  if not text:\n",
    "    return []\n",
    "  tokens = text.split()\n",
    "  return tokens\n",
    "\n",
    "\n",
    "class FullTokenizer(object):\n",
    "  \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
    "\n",
    "  def __init__(self, vocab_file, do_lower_case=True):\n",
    "    self.vocab = load_vocab(vocab_file)\n",
    "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
    "    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    split_tokens = []\n",
    "    for token in self.basic_tokenizer.tokenize(text):\n",
    "      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "        split_tokens.append(sub_token)\n",
    "\n",
    "    return split_tokens\n",
    "\n",
    "  def convert_tokens_to_ids(self, tokens):\n",
    "    return convert_by_vocab(self.vocab, tokens)\n",
    "\n",
    "  def convert_ids_to_tokens(self, ids):\n",
    "    return convert_by_vocab(self.inv_vocab, ids)\n",
    "\n",
    "\n",
    "class BasicTokenizer(object):\n",
    "  \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
    "\n",
    "  def __init__(self, do_lower_case=True):\n",
    "    \"\"\"Constructs a BasicTokenizer.\n",
    "\n",
    "    Args:\n",
    "      do_lower_case: Whether to lower case the input.\n",
    "    \"\"\"\n",
    "    self.do_lower_case = do_lower_case\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    \"\"\"Tokenizes a piece of text.\"\"\"\n",
    "    text = convert_to_unicode(text)\n",
    "    text = self._clean_text(text)\n",
    "\n",
    "    # This was added on November 1st, 2018 for the multilingual and Chinese\n",
    "    # models. This is also applied to the English models now, but it doesn't\n",
    "    # matter since the English models were not trained on any Chinese data\n",
    "    # and generally don't have any Chinese data in them (there are Chinese\n",
    "    # characters in the vocabulary because Wikipedia does have some Chinese\n",
    "    # words in the English Wikipedia.).\n",
    "    text = self._tokenize_chinese_chars(text)\n",
    "\n",
    "    orig_tokens = whitespace_tokenize(text)\n",
    "    split_tokens = []\n",
    "    for token in orig_tokens:\n",
    "      if self.do_lower_case:\n",
    "        token = token.lower()\n",
    "        token = self._run_strip_accents(token)\n",
    "      split_tokens.extend(self._run_split_on_punc(token))\n",
    "\n",
    "    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "    return output_tokens\n",
    "\n",
    "  def _run_strip_accents(self, text):\n",
    "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFD\", text)\n",
    "    output = []\n",
    "    for char in text:\n",
    "      cat = unicodedata.category(char)\n",
    "      if cat == \"Mn\":\n",
    "        continue\n",
    "      output.append(char)\n",
    "    return \"\".join(output)\n",
    "\n",
    "  def _run_split_on_punc(self, text):\n",
    "    \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
    "    chars = list(text)\n",
    "    i = 0\n",
    "    start_new_word = True\n",
    "    output = []\n",
    "    while i < len(chars):\n",
    "      char = chars[i]\n",
    "      if _is_punctuation(char):\n",
    "        output.append([char])\n",
    "        start_new_word = True\n",
    "      else:\n",
    "        if start_new_word:\n",
    "          output.append([])\n",
    "        start_new_word = False\n",
    "        output[-1].append(char)\n",
    "      i += 1\n",
    "\n",
    "    return [\"\".join(x) for x in output]\n",
    "\n",
    "  def _tokenize_chinese_chars(self, text):\n",
    "    \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
    "    output = []\n",
    "    for char in text:\n",
    "      cp = ord(char)\n",
    "      if self._is_chinese_char(cp):\n",
    "        output.append(\" \")\n",
    "        output.append(char)\n",
    "        output.append(\" \")\n",
    "      else:\n",
    "        output.append(char)\n",
    "    return \"\".join(output)\n",
    "\n",
    "  def _is_chinese_char(self, cp):\n",
    "    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
    "    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
    "    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
    "    #\n",
    "    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
    "    # despite its name. The modern Korean Hangul alphabet is a different block,\n",
    "    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
    "    # space-separated words, so they are not treated specially and handled\n",
    "    # like the all of the other languages.\n",
    "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
    "        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
    "        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
    "        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
    "        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
    "        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
    "        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
    "        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
    "      return True\n",
    "\n",
    "    return False\n",
    "\n",
    "  def _clean_text(self, text):\n",
    "    \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
    "    output = []\n",
    "    for char in text:\n",
    "      cp = ord(char)\n",
    "      if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "        continue\n",
    "      if _is_whitespace(char):\n",
    "        output.append(\" \")\n",
    "      else:\n",
    "        output.append(char)\n",
    "    return \"\".join(output)\n",
    "\n",
    "\n",
    "class WordpieceTokenizer(object):\n",
    "  \"\"\"Runs WordPiece tokenziation.\"\"\"\n",
    "\n",
    "  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
    "    self.vocab = vocab\n",
    "    self.unk_token = unk_token\n",
    "    self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "  def tokenize(self, text):\n",
    "    \"\"\"Tokenizes a piece of text into its word pieces.\n",
    "\n",
    "    This uses a greedy longest-match-first algorithm to perform tokenization\n",
    "    using the given vocabulary.\n",
    "\n",
    "    For example:\n",
    "      input = \"unaffable\"\n",
    "      output = [\"un\", \"##aff\", \"##able\"]\n",
    "\n",
    "    Args:\n",
    "      text: A single token or whitespace separated tokens. This should have\n",
    "        already been passed through `BasicTokenizer.\n",
    "\n",
    "    Returns:\n",
    "      A list of wordpiece tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    text = convert_to_unicode(text)\n",
    "\n",
    "    output_tokens = []\n",
    "    for token in whitespace_tokenize(text):\n",
    "      chars = list(token)\n",
    "      if len(chars) > self.max_input_chars_per_word:\n",
    "        output_tokens.append(self.unk_token)\n",
    "        continue\n",
    "\n",
    "      is_bad = False\n",
    "      start = 0\n",
    "      sub_tokens = []\n",
    "      while start < len(chars):\n",
    "        end = len(chars)\n",
    "        cur_substr = None\n",
    "        while start < end:\n",
    "          substr = \"\".join(chars[start:end])\n",
    "          if start > 0:\n",
    "            substr = \"##\" + substr\n",
    "          if substr in self.vocab:\n",
    "            cur_substr = substr\n",
    "            break\n",
    "          end -= 1\n",
    "        if cur_substr is None:\n",
    "          is_bad = True\n",
    "          break\n",
    "        sub_tokens.append(cur_substr)\n",
    "        start = end\n",
    "\n",
    "      if is_bad:\n",
    "        output_tokens.append(self.unk_token)\n",
    "      else:\n",
    "        output_tokens.extend(sub_tokens)\n",
    "    return output_tokens\n",
    "\n",
    "\n",
    "def _is_whitespace(char):\n",
    "  \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
    "  # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
    "  # as whitespace since they are generally considered as such.\n",
    "  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "    return True\n",
    "  cat = unicodedata.category(char)\n",
    "  if cat == \"Zs\":\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "\n",
    "def _is_control(char):\n",
    "  \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
    "  # These are technically control characters but we count them as whitespace\n",
    "  # characters.\n",
    "  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
    "    return False\n",
    "  cat = unicodedata.category(char)\n",
    "  if cat in (\"Cc\", \"Cf\"):\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "\n",
    "def _is_punctuation(char):\n",
    "  \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
    "  cp = ord(char)\n",
    "  # We treat all non-letter/number ASCII as punctuation.\n",
    "  # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "  # Punctuation class but we treat them as punctuation anyways, for\n",
    "  # consistency.\n",
    "  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
    "      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
    "    return True\n",
    "  cat = unicodedata.category(char)\n",
    "  if cat.startswith(\"P\"):\n",
    "    return True\n",
    "  return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# Copyright Tor Vergata, University of Rome. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "#\n",
    "# Data processor for the QC dataset\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "class InputExample(object):\n",
    "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "  def __init__(self, guid, text_a, text_b=None, label=None, \n",
    "              label_mask=None):\n",
    "    \"\"\"Constructs a InputExample.\n",
    "\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "    self.guid = guid\n",
    "    self.text_a = text_a\n",
    "    self.text_b = text_b\n",
    "    self.label = label\n",
    "    self.label_mask = label_mask\n",
    "\n",
    "\n",
    "class PaddingInputExample(object):\n",
    "  \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "  \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               label_id,\n",
    "               label_mask=1,\n",
    "               is_real_example=True):\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.label_id = label_id\n",
    "    self.is_real_example = is_real_example\n",
    "    self.label_mask = label_mask\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "  \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "  \n",
    "  def __init__(self, double_unordered):\n",
    "    self.double_unordered = True\n",
    "\n",
    "  def get_examples(self, data_dir, split):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return self._create_examples(\n",
    "        self._read_tsv(os.path.join(data_dir, split + \".tsv\")), split)\n",
    "\n",
    "  def _create_examples(self, lines, split):\n",
    "    pass \n",
    "\n",
    "  def get_labeled_examples(self, input_examples, label_ratio):\n",
    "    \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "    label_dict = OrderedDict({})\n",
    "    for index, example in enumerate(input_examples):\n",
    "      if example.label not in label_dict:\n",
    "        label_dict[example.label] = []\n",
    "      label_dict[example.label].append(index)\n",
    "\n",
    "    labeled_examples = []\n",
    "    for label in label_dict:\n",
    "      label_count = len(label_dict[label])\n",
    "      label_example = label_dict[label][0:int(label_count*label_ratio)]\n",
    "      for index in label_example:\n",
    "        input_examples[index].label_mask = 1\n",
    "        labeled_examples.append(input_examples[index])\n",
    "\n",
    "    random.shuffle(labeled_examples)\n",
    "    return labeled_examples\n",
    "\n",
    "  def get_unlabeled_examples(self, input_examples, label_ratio):\n",
    "    \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "    label_dict = OrderedDict({})\n",
    "    for index, example in enumerate(input_examples):\n",
    "      if example.label not in label_dict:\n",
    "        label_dict[example.label] = []\n",
    "      label_dict[example.label].append(index)\n",
    "\n",
    "    unlabeled_examples = []\n",
    "    for label in label_dict:\n",
    "      label_count = len(label_dict[label])\n",
    "      unlabel_example = label_dict[label][int(label_count*label_ratio):]\n",
    "      for index in unlabel_example:\n",
    "        input_examples[index].label_mask = 0\n",
    "        unlabeled_examples.append(input_examples[index])\n",
    "    random.shuffle(unlabeled_examples)\n",
    "    return unlabeled_examples\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def _get_dummy_label(self):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  @classmethod\n",
    "  def _read_tsv(cls, input_file, quotechar=None):\n",
    "    \"\"\"Reads a tab separated value file.\"\"\"\n",
    "    with tf.gfile.GFile(input_file, \"r\") as f:\n",
    "      reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "      lines = []\n",
    "      for line in reader:\n",
    "        lines.append(line)\n",
    "      random.shuffle(lines)\n",
    "      return lines\n",
    "\n",
    "  def _load_glue(self, lines, split, text_a_loc, text_b_loc, label_loc,\n",
    "                 name,\n",
    "                 skip_first_line=False, eid_offset=0, swap=False):\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "      try:\n",
    "        if i == 0 and skip_first_line:\n",
    "          continue\n",
    "        eid = i - (1 if skip_first_line else 0) + eid_offset\n",
    "        text_a = convert_to_unicode(line[text_a_loc])\n",
    "        if text_b_loc is None:\n",
    "          text_b = None\n",
    "        else:\n",
    "          text_b = convert_to_unicode(line[text_b_loc])\n",
    "        if \"test\" in split or \"diagnostic\" in split:\n",
    "          label = self._get_dummy_label()\n",
    "        else:\n",
    "          label = convert_to_unicode(line[label_loc])\n",
    "        if swap:\n",
    "          text_a, text_b = text_b, text_a\n",
    "        examples.append(InputExample(guid=eid, text_a=text_a, text_b=text_b, label=label))\n",
    "      except Exception as ex:\n",
    "        tf.logging.info(\"Error constructing example from line\", i,\n",
    "                  \"for task\", name + \":\", ex)\n",
    "        tf.logging.info(\"Input causing the error:\", line)\n",
    "    random.shuffle(examples)\n",
    "    return examples\n",
    "\n",
    "class QcFineProcessor(DataProcessor):\n",
    "  \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
    "\n",
    "  def __init__(self, double_unordered):\n",
    "    super(QcFineProcessor, self).__init__(double_unordered)\n",
    "\n",
    "  def get_labeled_examples(self, data_dir):\n",
    "      \"\"\"See base class.\"\"\"\n",
    "      return self._create_examples(os.path.join(data_dir, \"labeled.tsv\"), \"train\")\n",
    "\n",
    "  def get_unlabeled_examples(self, data_dir):\n",
    "      \"\"\"See base class.\"\"\"\n",
    "      return self._create_examples(os.path.join(data_dir, \"unlabeled.tsv\"), \"train\")\n",
    "\n",
    "  def get_test_examples(self, data_dir):\n",
    "      \"\"\"See base class.\"\"\"\n",
    "      return self._create_examples(os.path.join(data_dir, \"test.tsv\"), \"test\")\n",
    "\n",
    "  def get_labels(self):\n",
    "      \"\"\"See base class.\"\"\"\n",
    "      return [\"UNK_UNK\", \"ABBR_abb\", \"ABBR_exp\", \"DESC_def\", \"DESC_desc\", \"DESC_manner\", \"DESC_reason\", \"ENTY_animal\", \"ENTY_body\", \"ENTY_color\", \"ENTY_cremat\", \"ENTY_currency\", \"ENTY_dismed\", \"ENTY_event\", \"ENTY_food\", \"ENTY_instru\", \"ENTY_lang\", \"ENTY_letter\", \"ENTY_other\", \"ENTY_plant\", \"ENTY_product\", \"ENTY_religion\", \"ENTY_sport\", \"ENTY_substance\", \"ENTY_symbol\", \"ENTY_techmeth\", \"ENTY_termeq\", \"ENTY_veh\", \"ENTY_word\", \"HUM_desc\", \"HUM_gr\", \"HUM_ind\", \"HUM_title\", \"LOC_city\", \"LOC_country\", \"LOC_mount\", \"LOC_other\", \"LOC_state\", \"NUM_code\", \"NUM_count\", \"NUM_date\", \"NUM_dist\", \"NUM_money\", \"NUM_ord\", \"NUM_other\", \"NUM_perc\", \"NUM_period\", \"NUM_speed\", \"NUM_temp\", \"NUM_volsize\", \"NUM_weight\"]\n",
    "\n",
    "  def _create_examples(self, input_file, split):\n",
    "      \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "      examples = []\n",
    "\n",
    "      with tf.gfile.GFile(input_file, \"r\") as f:\n",
    "          contents = f.read()\n",
    "          file_as_list = contents.splitlines()\n",
    "          for line in file_as_list[1:]:\n",
    "              split = line.split(\" \")\n",
    "              question = ' '.join(split[1:])\n",
    "\n",
    "              guid = \"%s-%s\" % (split, tokenization.convert_to_unicode(line))\n",
    "              text_a = tokenization.convert_to_unicode(question)\n",
    "              inn_split = split[0].split(\":\")\n",
    "              label = inn_split[0] + \"_\" + inn_split[1]\n",
    "              examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "\n",
    "      random.shuflle(examples)\n",
    "      return examples\n",
    "\n",
    "  def _get_dummy_label(self):\n",
    "    labels = self.get_labels()\n",
    "    return labels[0]\n",
    "\n",
    "class SST(DataProcessor):\n",
    "  \"\"\"Stanford Sentiment Treebank.\"\"\"\n",
    "  def __init__(self, double_unordered):\n",
    "    super(SST, self).__init__(double_unordered)\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"0\", \"1\"]\n",
    "\n",
    "  def _get_dummy_label(self):\n",
    "    labels = self.get_labels()\n",
    "    return labels[0]\n",
    "\n",
    "  def get_test_splits(self):\n",
    "    return [\"test\"]\n",
    "\n",
    "  def get_dev_splits(self):\n",
    "    return [\"dev\"]\n",
    "\n",
    "  def _create_examples(self, lines, split):\n",
    "    if \"test\" in split:\n",
    "      return self._load_glue(lines, split, 1, None, None, True)\n",
    "    else:\n",
    "      return self._load_glue(lines, split, 0, None, 1, True)\n",
    "\n",
    "class MNLI(DataProcessor):\n",
    "  \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
    "  def __init__(self, double_unordered):\n",
    "    super(MNLI, self).__init__(double_unordered)\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "\n",
    "  def _get_dummy_label(self):\n",
    "    labels = self.get_labels()\n",
    "    return labels[0]\n",
    "\n",
    "  def _create_examples(self, lines, split):\n",
    "    if split == \"diagnostic\":\n",
    "      return self._load_glue(lines, split, 1, 2, None, True)\n",
    "    else:\n",
    "      return self._load_glue(lines, split, 8, 9, -1, True)\n",
    "\n",
    "  def get_test_splits(self):\n",
    "    return [\"test_matched\", \"test_mismatched\", \"diagnostic\"]\n",
    "\n",
    "  def get_dev_splits(self):\n",
    "    return [\"dev_matched\", \"dev_mismatched\"]\n",
    "\n",
    "class WNLI(DataProcessor):\n",
    "  \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
    "  def __init__(self, double_unordered):\n",
    "    super(WNLI, self).__init__(double_unordered)\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"0\", \"1\"]\n",
    "\n",
    "  def _get_dummy_label(self):\n",
    "    labels = self.get_labels()\n",
    "    return labels[0]\n",
    "\n",
    "  def get_test_splits(self):\n",
    "    return [\"test\"]\n",
    "\n",
    "  def get_dev_splits(self):\n",
    "    return [\"dev\"]\n",
    "\n",
    "  def _create_examples(self, lines, split):\n",
    "    if \"test\" in split:\n",
    "      return self._load_glue(lines, split, 1, 2, None, True)\n",
    "    else:\n",
    "      return self._load_glue(lines, split, 1, 2, -1, True)\n",
    "\n",
    "class MRPC(DataProcessor):\n",
    "  \"\"\"Processor for the MRPC data set (GLUE version).\"\"\"\n",
    "  def __init__(self, double_unordered=True):\n",
    "    super(MRPC, self).__init__(double_unordered)\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"0\", \"1\"]\n",
    "\n",
    "  def _get_dummy_label(self):\n",
    "    labels = self.get_labels()\n",
    "    return labels[0]\n",
    "\n",
    "  def get_test_splits(self):\n",
    "    return [\"test\"]\n",
    "\n",
    "  def get_dev_splits(self):\n",
    "    return [\"dev\"]\n",
    "\n",
    "  def _create_examples(self, lines, split):\n",
    "    examples = []\n",
    "    examples += self._load_glue(lines, split, 3, 4, 0, True)\n",
    "    if self.double_unordered and split == \"train\":\n",
    "      examples += self._load_glue(\n",
    "          lines, split, 3, 4, 0, True, len(examples), True)\n",
    "    return examples\n",
    "\n",
    "class COLA(DataProcessor):\n",
    "  \"\"\"Processor for the CoLA data set (GLUE version).\"\"\"\n",
    "  def __init__(self, double_unordered=False):\n",
    "    super(COLA, self).__init__(double_unordered)\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"0\", \"1\"]\n",
    "\n",
    "  def _get_dummy_label(self):\n",
    "    labels = self.get_labels()\n",
    "    return labels[0]\n",
    "\n",
    "  def get_test_splits(self):\n",
    "    return [\"test\"]\n",
    "\n",
    "  def get_dev_splits(self):\n",
    "    return [\"dev\"]\n",
    "\n",
    "  def _create_examples(self, lines, split):\n",
    "    return self._load_glue(lines, split, 1 if split == \"test\" else 3,\n",
    "                           None, 1, split == \"test\")\n",
    "\n",
    "class QQP(DataProcessor):\n",
    "  \"\"\"Processor for the CoLA data set (GLUE version).\"\"\"\n",
    "  def __init__(self, double_unordered=True):\n",
    "    super(QQP, self).__init__(double_unordered)\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"0\", \"1\"]\n",
    "\n",
    "  def _get_dummy_label(self):\n",
    "    labels = self.get_labels()\n",
    "    return labels[0]\n",
    "\n",
    "  def get_test_splits(self):\n",
    "    return [\"test\"]\n",
    "\n",
    "  def get_dev_splits(self):\n",
    "    return [\"dev\"]\n",
    "\n",
    "  def _create_examples(self, lines, split):\n",
    "    return self._load_glue(lines, split, 1 if split == \"test\" else 3,\n",
    "                           2 if split == \"test\" else 4, 5, True)\n",
    "\n",
    "class RTE(DataProcessor):\n",
    "  \"\"\"Recognizing Textual Entailment.\"\"\"\n",
    "  def __init__(self, double_unordered=False):\n",
    "    super(RTE, self).__init__(double_unordered)\n",
    "  \n",
    "  def _create_examples(self, lines, split):\n",
    "    return self._load_glue(lines, split, 1, 2, 3, True)\n",
    "\n",
    "  def _get_dummy_label(self):\n",
    "    labels = self.get_labels()\n",
    "    return labels[0]\n",
    "\n",
    "  def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"entailment\", \"not_entailment\"]\n",
    "\n",
    "  def get_test_splits(self):\n",
    "    return [\"test\"]\n",
    "\n",
    "  def get_dev_splits(self):\n",
    "    return [\"dev\"]\n",
    " \n",
    "class QNLI(DataProcessor):\n",
    "  \"\"\"Question NLI.\"\"\"\n",
    "  def __init__(self, double_unordered=False):\n",
    "    super(QNLI, self).__init__(double_unordered)\n",
    "  \n",
    "  def _create_examples(self, lines, split):\n",
    "    return self._load_glue(lines, split, 1, 2, 3, True)\n",
    "\n",
    "  def _get_dummy_label(self):\n",
    "    labels = self.get_labels()\n",
    "    return labels[0]\n",
    "\n",
    "  def get_labels(self):\n",
    "    return [\"entailment\", \"not_entailment\"]\n",
    "\n",
    "  def get_test_splits(self):\n",
    "    return [\"test\"]\n",
    "\n",
    "  def get_dev_splits(self):\n",
    "    return [\"dev\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnli = QNLI(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CoLA', '.DS_Store', 'SST-2', 'diagnostic', 'WNLI', 'QQP', 'MRPC', 'MNLI', 'STS-B', 'QNLI', 'RTE']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('/data/xuht/glue/'))\n",
    "# exampls = qnli.get_examples('/data/xuht/glue/QNLI/', 'test' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Succeeded in testing:CoLA **\n",
      "** Succeeded in testing:WNLI **\n",
      "** Succeeded in testing:QQP **\n",
      "** Succeeded in testing:MRPC **\n",
      "** Succeeded in testing:MNLI **\n",
      "** Succeeded in testing:STS-B **\n",
      "** Succeeded in testing:QNLI **\n",
      "** Succeeded in testing:RTE **\n"
     ]
    }
   ],
   "source": [
    "task_mapping = {\n",
    "    'CoLA':COLA,\n",
    "    'STS-B': SST,\n",
    "    'WNLI': WNLI,\n",
    "    'QQP': QQP,\n",
    "    'MRPC': MRPC,\n",
    "    'MNLI': MNLI,\n",
    "    'QNLI': QNLI,\n",
    "    'RTE': RTE\n",
    "}\n",
    "for task in os.listdir('/data/xuht/glue/'):\n",
    "    if task not in task_mapping:\n",
    "        continue\n",
    "    api = task_mapping[task](False)\n",
    "    train_examples = api.get_examples('/data/xuht/glue/'+task, 'train' )\n",
    "    unlabeled = api.get_unlabeled_examples(train_examples, 0.1)\n",
    "    labeled = api.get_labeled_examples(train_examples, 0.1)\n",
    "    for dev in api.get_dev_splits():\n",
    "        dev_examples = api.get_examples('/data/xuht/glue/'+task, dev )\n",
    "    for test in api.get_test_splits():\n",
    "        test_examples = api.get_examples('/data/xuht/glue/'+task, dev )\n",
    "    \n",
    "        \n",
    "    print(\"** Succeeded in testing:%s **\"%(task))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2243, 248, 2491)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlabeled), len(labeled), len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Even while accepting the Russian plan, IMF Managing Director Michel Camdessus noted that the efficiency of Russia\\'s State Taxation Service \"is declining rapidly.\"',\n",
       " 'Michel Camdessus is managing director of IMF.',\n",
       " 'entailment',\n",
       " 1998)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled[0].text_a, unlabeled[0].text_b, unlabeled[0].label, unlabeled[0].guid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.InputExample object at 0x7f57bccf95c0>\n"
     ]
    }
   ],
   "source": [
    "for example in train_examples:\n",
    "    if example.guid == 1998:\n",
    "        print(example)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Even while accepting the Russian plan, IMF Managing Director Michel Camdessus noted that the efficiency of Russia\\'s State Taxation Service \"is declining rapidly.\"',\n",
       " 'Michel Camdessus is managing director of IMF.',\n",
       " 0)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.text_a, example.text_b, example.label_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
