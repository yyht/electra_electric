{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "Adapted from https://github.com/gpeyre/SinkhornAutoDiff\n",
    "and from https://github.com/dfdazac/wassdistance/blob/master/layers.py\n",
    "and from https://github.com/michaelsdr/sinkformers/blob/main/nlp-tutorial/text-classification-transformer/sinkhorn.py\n",
    "\"\"\"\n",
    "\n",
    "def shape_list(x, out_type=tf.int32):\n",
    "  \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "  static = x.shape.as_list()\n",
    "  dynamic = tf.shape(x, out_type=out_type)\n",
    "  return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "def sinkhorn_distance(input_tensor, eps, max_iter, \n",
    "                  reduction='none',\n",
    "                  stopThr=1e-2):\n",
    "  \n",
    "  C = input_tensor\n",
    "  C_shape = shape_list(C)\n",
    "\n",
    "  x_points = C_shape[-2]\n",
    "  y_points = C_shape[-1]\n",
    "  batch_size = C_shape[0]\n",
    "    \n",
    "  # both marginals are fixed with equal weights\n",
    "  mu = 1.0 / x_points * tf.ones((batch_size, x_points))\n",
    "  nu = 1.0 / y_points * tf.ones((batch_size, y_points))\n",
    "\n",
    "  u = tf.zeros_like(mu)\n",
    "  v = tf.zeros_like(nu)\n",
    "\n",
    "  cpt = tf.constant(0)\n",
    "  err = tf.constant(1.0)\n",
    "\n",
    "  c = lambda cpt, u, v, err: tf.logical_and(cpt < max_iter, err > stopThr)\n",
    "\n",
    "  def M( C, u, v):\n",
    "    \"Modified cost for logarithmic updates\"\n",
    "    \"$M_{ij} = (-c_{ij} + u_i + v_j) / \\epsilon$\"\n",
    "    return (-C + tf.expand_dims(u, -1) + tf.expand_dims(v, -2) )/eps\n",
    "\n",
    "  def loop_func(cpt, u, v, err):\n",
    "    u1 = tf.identity(u)  # useful to check the update\n",
    "\n",
    "    cpt = cpt + 1\n",
    "\n",
    "    u = eps * (tf.log(mu+1e-8) - tf.reduce_logsumexp(M(C, u, v), axis=-1)) + u\n",
    "    v = eps * (tf.log(nu+1e-8) - tf.reduce_logsumexp(tf.transpose(M(C, u, v), [0, 2, 1]), axis=-1)) + v\n",
    "\n",
    "    err = tf.reduce_mean(tf.reduce_sum(tf.abs(u - u1), axis=-1))\n",
    "\n",
    "    return cpt, u, v, err\n",
    "\n",
    "  _, u_final, v_final, _ = tf.while_loop(c, loop_func, loop_vars=[cpt, u, v, err])\n",
    "  U, V = tf.identity(u_final), tf.identity(v_final)\n",
    "\n",
    "  # Transport plan pi = diag(a)*K*diag(b)\n",
    "  pi = tf.exp(M(C, U, V))\n",
    "\n",
    "  cost = tf.reduce_sum(pi * C, axis=(-2, -1))\n",
    "\n",
    "  return pi, C, U, V, cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1.0\n",
    "max_iter = 10\n",
    "stopThr = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cost_matrix(x, y, p=2):\n",
    "    \"Returns the matrix of $|x_i-y_j|^p$.\"\n",
    "    x_col = tf.expand_dims(x, axis=-2)\n",
    "    y_lin = tf.expand_dims(y, axis=-3)\n",
    "    C = tf.reduce_sum((tf.abs(x_col - y_lin)) ** p, -1)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((1, 10, 32)).astype(np.float32)\n",
    "y = np.random.random((1, 16, 32)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = _cost_matrix(tf.constant(x), tf.constant(y), p=2)\n",
    "\n",
    "[pi, C_, U, V, final_cost] = sinkhorn_distance(C, eps, max_iter, \n",
    "                  reduction='none',\n",
    "                  stopThr=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "resp = sess.run([pi, C, U, V, final_cost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06250001, 0.06250001, 0.06250001, 0.0625    , 0.06250001,\n",
       "        0.06250001, 0.06250002, 0.06250002, 0.06250003, 0.06250001,\n",
       "        0.06250003, 0.06250001, 0.06250001, 0.06250001, 0.0625    ,\n",
       "        0.06250001]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp[0].sum(axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Adapted from https://github.com/gpeyre/SinkhornAutoDiff\n",
    "class SinkhornDistance(nn.Module):\n",
    "    r\"\"\"\n",
    "    Given two empirical measures each with :math:`P_1` locations\n",
    "    :math:`x\\in\\mathbb{R}^{D_1}` and :math:`P_2` locations :math:`y\\in\\mathbb{R}^{D_2}`,\n",
    "    outputs an approximation of the regularized OT cost for point clouds.\n",
    "    Args:\n",
    "        eps (float): regularization coefficient\n",
    "        max_iter (int): maximum number of Sinkhorn iterations\n",
    "        reduction (string, optional): Specifies the reduction to apply to the output:\n",
    "            'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n",
    "            'mean': the sum of the output will be divided by the number of\n",
    "            elements in the output, 'sum': the output will be summed. Default: 'none'\n",
    "    Shape:\n",
    "        - Input: :math:`(N, P_1, D_1)`, :math:`(N, P_2, D_2)`\n",
    "        - Output: :math:`(N)` or :math:`()`, depending on `reduction`\n",
    "    \"\"\"\n",
    "    def __init__(self, eps, max_iter, reduction='none'):\n",
    "        super(SinkhornDistance, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.max_iter = max_iter\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # The Sinkhorn algorithm takes as input three variables :\n",
    "        C = self._cost_matrix(x, y)  # Wasserstein cost function\n",
    "        x_points = x.shape[-2]\n",
    "        y_points = y.shape[-2]\n",
    "        if x.dim() == 2:\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            batch_size = x.shape[0]\n",
    "\n",
    "        # both marginals are fixed with equal weights\n",
    "        mu = torch.empty(batch_size, x_points, dtype=torch.float,\n",
    "                         requires_grad=False).fill_(1.0 / x_points).squeeze()\n",
    "        nu = torch.empty(batch_size, y_points, dtype=torch.float,\n",
    "                         requires_grad=False).fill_(1.0 / y_points).squeeze()\n",
    "\n",
    "        u = torch.zeros_like(mu)\n",
    "        v = torch.zeros_like(nu)\n",
    "        # To check if algorithm terminates because of threshold\n",
    "        # or max iterations reached\n",
    "        actual_nits = 0\n",
    "        # Stopping criterion\n",
    "        thresh = 1e-1\n",
    "\n",
    "        # Sinkhorn iterations\n",
    "        for i in range(self.max_iter):\n",
    "            u1 = u  # useful to check the update\n",
    "            u = self.eps * (torch.log(mu+1e-8) - torch.logsumexp(self.M(C, u, v), dim=-1)) + u\n",
    "            v = self.eps * (torch.log(nu+1e-8) - torch.logsumexp(self.M(C, u, v).transpose(-2, -1), dim=-1)) + v\n",
    "            err = (u - u1).abs().sum(-1).mean()\n",
    "\n",
    "            actual_nits += 1\n",
    "            if err.item() < thresh:\n",
    "                break\n",
    "\n",
    "        U, V = u, v\n",
    "        # Transport plan pi = diag(a)*K*diag(b)\n",
    "        pi = torch.exp(self.M(C, U, V))\n",
    "        # Sinkhorn distance\n",
    "        cost = torch.sum(pi * C, dim=(-2, -1))\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            cost = cost.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            cost = cost.sum()\n",
    "\n",
    "        return cost, pi, C\n",
    "\n",
    "    def M(self, C, u, v):\n",
    "        \"Modified cost for logarithmic updates\"\n",
    "        \"$M_{ij} = (-c_{ij} + u_i + v_j) / \\epsilon$\"\n",
    "        return (-C + u.unsqueeze(-1) + v.unsqueeze(-2)) / self.eps\n",
    "\n",
    "    @staticmethod\n",
    "    def _cost_matrix(x, y, p=2):\n",
    "        \"Returns the matrix of $|x_i-y_j|^p$.\"\n",
    "        x_col = x.unsqueeze(-2)\n",
    "        y_lin = y.unsqueeze(-3)\n",
    "        C = torch.sum((torch.abs(x_col - y_lin)) ** p, -1)\n",
    "        return C\n",
    "\n",
    "    @staticmethod\n",
    "    def ave(u, u1, tau):\n",
    "        \"Barycenter subroutine, used by kinetic acceleration through extrapolation.\"\n",
    "        return tau * u + (1 - tau) * u1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sink = SinkhornDistance(eps=eps, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cost, pi, C = sink.forward(torch.tensor(x), torch.tensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "      CharBPETokenizer,\n",
    "      SentencePieceBPETokenizer,\n",
    "      BertWordPieceTokenizer)\n",
    "\n",
    "vocab = '/data/xuht/uncased_L-12_H-768_A-12_ilm_v1/vocab_uncased_en.txt'\n",
    "\n",
    "chinese_bpe_tokenizer = BertWordPieceTokenizer(\n",
    "    vocab, \n",
    "    lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method decode in module tokenizers.implementations.base_tokenizer:\n",
      "\n",
      "decode(ids:List[int], skip_special_tokens:Union[bool, NoneType]=True) -> str method of tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer instance\n",
      "    Decode the given list of ids to a string sequence\n",
      "    \n",
      "    Args:\n",
      "        ids: List[unsigned int]:\n",
      "            A list of ids to be decoded\n",
      "    \n",
      "        skip_special_tokens: (`optional`) boolean:\n",
      "            Whether to remove all the special tokens from the output string\n",
      "    \n",
      "    Returns:\n",
      "        The decoded string\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(chinese_bpe_tokenizer.decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.not_equal([1.,2.,3.,0.], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing\n",
    "a = [1,0,3]\n",
    "label_binarizer = sklearn.preprocessing.LabelBinarizer()\n",
    "label_binarizer.fit(range(max(a)+1))\n",
    "b = label_binarizer.transform(a)\n",
    "print('{0}'.format(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.random.randint(1, 4, size=[2,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tf = tf.one_hot(label, depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_label = sess.run(label_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 1, 3, 3],\n",
       "       [2, 3, 1, 2, 2]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0],\n",
       "       [0, 1, 0, 1],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 1],\n",
       "       [0, 0, 1, 1]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(one_hot_label.sum(axis=0) !=0)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
